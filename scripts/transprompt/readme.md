
# TransPrompt

Our proposed TransPrompt is motivated by the join of prompt-tuning and cross-task transfer learning. 
The aim is to explore and exploit the transferable knowledge from similar tasks in the few-shot scenario, 
and make the Pre-trained Language Model (PLM) better few-shot transfer learner.
Our proposed framework is accepted by the main conference (long paper track) in EMNLP-2021[《TransPrompt：Towards an Automatic Transferable Prompting Framework for Few-shot Text Classification》](https://aclanthology.org/2021.emnlp-main.221/)

This code is re-implement in [EasyNLP](https://github.com/alibaba/EasyNLP/tree/master/examples/transprompt).


### Citation
Our paper citation is:

```
@inproceedings{DBLP:conf/emnlp/0001WQH021,
  author    = {Chengyu Wang and
               Jianing Wang and
               Minghui Qiu and
               Jun Huang and
               Ming Gao},
  title     = {TransPrompt: Towards an Automatic Transferable Prompting Framework
               for Few-shot Text Classification},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural
               Language Processing, {EMNLP} 2021, Virtual Event / Punta Cana, Dominican
               Republic, 7-11 November, 2021},
  pages     = {2792--2802},
  publisher = {Association for Computational Linguistics},
  year      = {2021},
  url       = {https://aclanthology.org/2021.emnlp-main.221},
  timestamp = {Tue, 09 Nov 2021 13:51:50 +0100},
  biburl    = {https://dblp.org/rec/conf/emnlp/0001WQH021.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
```
