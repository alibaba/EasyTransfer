# Meta-KD: A Meta Knowledge Distillation Framework for Language Model Compression across Domains

The code has been re-implemented with EasyNLP. Please [check it out here](https://github.com/alibaba/EasyNLP/tree/master/examples/knowledge_distillation) 

If you use the code, please cite the following paper. Thanks.

```
@article{pan2021metakd,
  author    = {Haojie Pan and
               Chengyu Wang and
               Minghui Qiu and
               Yichang Zhang and
               Yaliang Li and
               Jun Huang},
  title     = {Meta-KD: A Meta Knowledge Distillation Framework for Language Model
               Compression across Domains},
  journal   = {ACL},
  year      = {2021}
}
```
